{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark\n",
    "* use spark.ml package \n",
    "* works on dataFrame \n",
    "* works as pipeline model\n",
    "\n",
    "#### Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [content: string, sentiment: int]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read\n",
    ".format(\"csv\")\n",
    ".option(\"inferSchema\",true)\n",
    ".option(\"header\",true)\n",
    ".csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             content|sentiment|\n",
      "+--------------------+---------+\n",
      "|Great fun!, Got t...|        1|\n",
      "|Inspiring, I hope...|        1|\n",
      "|Great CD, My love...|        1|\n",
      "|First album I've ...|        1|\n",
      "|Amazing!, I used ...|        1|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 3000|\n",
      "|        0|30000|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"sentiment\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newNames: Seq[String] = List(text, label)\r\n",
       "dataRenamed: org.apache.spark.sql.DataFrame = [text: string, label: int]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Renaming content-> text and sentiment->label \n",
    "val newNames = Seq(\"text\",\"label\")\n",
    "val dataRenamed = data.toDF(newNames: _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|Great fun!, Got t...|    1|\n",
      "|Inspiring, I hope...|    1|\n",
      "|Great CD, My love...|    1|\n",
      "|First album I've ...|    1|\n",
      "|Amazing!, I used ...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataRenamed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [text: string, label: int]\r\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [text: string, label: int]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//training test split\n",
    "val Array(trainData, testData) = dataRenamed.randomSplit(Array(0.8,0.2), seed=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 2400|\n",
      "|    0|23975|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainData.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Long = 26375\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Long = 6625\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\r\n",
       "import org.apache.spark.ml.classification.LogisticRegression\r\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\r\n",
       "import org.apache.spark.ml.feature.{HashingTF, StopWordsRemover, IDF, Tokenizer}\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{HashingTF, StopWordsRemover, IDF, Tokenizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tok: org.apache.spark.ml.feature.Tokenizer = tok_aa87c1b0a09b\r\n",
       "remove: org.apache.spark.ml.feature.StopWordsRemover = stopWords_03d79aefc048\r\n",
       "hashedTF: org.apache.spark.ml.feature.HashingTF = hashingTF_779abaed4d9f\r\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_9dcbac74602a\r\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_105391d32b73\r\n",
       "pipeLine: org.apache.spark.ml.Pipeline = pipeline_6459801cb227\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//tokenizer\n",
    "val tok = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "//remove stop words\n",
    "val remove = new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(false)\n",
    "//hashign or TF\n",
    "val hashedTF = new HashingTF().setNumFeatures(5000).setInputCol(\"filtered\").setOutputCol(\"features\")\n",
    "//IDF\n",
    "val idf = new IDF().setInputCol(\"features\").setOutputCol(\"normalizedFeatures\").setMinDocFreq(0)\n",
    "// classifier \n",
    "val lr = new LogisticRegression().setRegParam(0.01).setThreshold(0.5)\n",
    "// Pipeline all\n",
    "val pipeLine = new Pipeline().setStages(Array(tok, remove, hashedTF, idf, lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[String] = Array(i, me, my, myself, we)\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove.getStopWords.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_6459801cb227\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeLine.fit(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted: org.apache.spark.sql.DataFrame = [text: string, label: int ... 7 more fields]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predicted = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+\n",
      "|                text|label|         probability|prediction|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "|!!!! Wrong MP3 fi...|    0|[0.99489302164069...|       0.0|\n",
      "|&lt;3, I'm not a ...|    1|[0.28354425889542...|       1.0|\n",
      "|&quot;Definitely ...|    0|[0.97141251894541...|       0.0|\n",
      "|&quot;Takin' Back...|    0|[0.81497603297694...|       0.0|\n",
      "|&quot;Twilight of...|    0|[0.88131922176733...|       0.0|\n",
      "|*** JEAN HILL IS ...|    1|[0.82651317141594...|       0.0|\n",
      "|*WARNING* DO NOT ...|    0|[0.99767830777980...|       0.0|\n",
      "|--------IGNORANCE...|    0|[0.99964429894223...|       0.0|\n",
      "|...a minor produc...|    0|[0.99992728796269...|       0.0|\n",
      "|1 set insufficien...|    0|[0.60845008314316...|       0.0|\n",
      "|1 time use only, ...|    0|[0.95895773992685...|       0.0|\n",
      "|1*2*3* Rent, Don'...|    0|[0.96561619878039...|       0.0|\n",
      "|1, and thats bein...|    0|[0.81198321856807...|       0.0|\n",
      "|100 years of tort...|    0|[0.64021980280992...|       0.0|\n",
      "|100 years to read...|    0|[0.99696085187365...|       0.0|\n",
      "|100% CR**!, What ...|    0|[0.98123401023322...|       0.0|\n",
      "|100% Satisfied, B...|    1|[0.63479405110522...|       0.0|\n",
      "|12 hour relief?!,...|    0|[0.99972297997610...|       0.0|\n",
      "|15 Minutes of Cab...|    0|[0.99933691338291...|       0.0|\n",
      "|17 Steps to teach...|    0|[0.85778635705125...|       0.0|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.select(\"text\",\"label\", \"probability\", \"prediction\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOC : 0.914860580912864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_830a0f484f9e\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "println(\"AOC : \" + evaluator.evaluate(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual_pred: org.apache.spark.sql.DataFrame = [label: int, prediction: double]\r\n",
       "diff: org.apache.spark.sql.DataFrame = [label: int, prediction: double ... 1 more field]\r\n",
       "misClassified: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [diff: double]\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//accuracy \n",
    "val actual_pred = predicted.select(\"label\",\"prediction\")\n",
    "val diff = actual_pred.withColumn(\"diff\",abs(actual_pred(\"label\") -actual_pred(\"prediction\")))\n",
    "val misClassified = diff.select(\"diff\").filter($\"diff\" > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "missed: Long = 413\r\n",
       "accuracy: Double = 94.13185564080705\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val missed = misClassified.count()\n",
    "val accuracy = 100.0*(1.0 -  missed.toDouble /( missed + diff.count()).toDouble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
